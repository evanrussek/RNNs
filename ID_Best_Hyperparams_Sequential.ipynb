{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a8f525f-ab49-4335-a019-57d4ff9ad541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "827fd2ce-0137-4696-b3a5-d28cf28cf05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param_folder = '/home/erussek/projects/RNNs/best_hyper_params_sequential'\n",
    "if not os.path.exists(best_param_folder):\n",
    "    os.mkdir(best_param_folder)\n",
    "best_param_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ce7d55a-f608-4f8f-ad4b-b03fd5580562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(run_idx, part_name,train_seq_part, fu, model_name, d_model, sim_lr, human_lr, n_head, n_layers):\n",
    "    \n",
    "    to_save_folder = '/scratch/gpfs/erussek/RNN_project/Hyper_Param_Search_Sequential'\n",
    "    res_name_full = '{}_{}_{}_run_{}_model_name_{}_d_model_{}_sim_lr_{}_human_lr_{}_n_head_{}_n_layers_{}'.format(part_name,train_seq_part, fu, run_idx, model_name, d_model, sim_lr, human_lr, n_head, n_layers)\n",
    "    param_dict = {'part_name': part_name, 'train_seq_part': train_seq_part, 'fu': fu, 'model_name':model_name, 'd_model':d_model, 'sim_lr':sim_lr, 'human_lr':human_lr, 'n_head':n_head, 'n_layers':n_layers}\n",
    "    res_file_name = res_name_full + '.pickle'\n",
    "    res_full_file_name = os.path.join(to_save_folder, res_file_name)\n",
    "    file = open(res_full_file_name, 'rb')\n",
    "    res = pickle.load(file)\n",
    "    return res, param_dict\n",
    "\n",
    "def load_results_all_runs(part_name,train_seq_part, fu, model_name, d_model, sim_lr, human_lr, n_head, n_layers, n_runs = 2):\n",
    "    \n",
    "    results_list = []\n",
    "    \n",
    "    for run_idx in range(n_runs):\n",
    "        res, param_dict = load_results(run_idx, part_name,train_seq_part, fu, model_name, d_model, sim_lr, human_lr, n_head, n_layers)\n",
    "        results_list.append(res)\n",
    "        \n",
    "    return results_list, param_dict\n",
    "\n",
    "def get_learning_curve(part_name,train_seq_part, fu, model_name, d_model, sim_lr, human_lr, n_head, n_layers, n_runs = 2, which_loss = 'simulation_loss_results'): \n",
    "    \n",
    "    results_list, param_dict = load_results_all_runs(part_name,train_seq_part, fu, model_name, d_model, sim_lr, human_lr, n_head, n_layers, n_runs = n_runs)\n",
    "    \n",
    "    loss_results_by_run = np.array([res[which_loss] for res in results_list])\n",
    "    \n",
    "    return np.mean(loss_results_by_run, axis=0), np.std(loss_results_by_run, axis=0)/np.sqrt(n_runs), results_list[0]['train_sequence_number'], results_list[0]['simulation_sequence_number'], results_list[0]['human_sequence_number'], param_dict\n",
    "\n",
    "def get_best_params(res_losses, res_params, simulation_sequence_number, human_sequence_number):\n",
    "    loss_arr = np.array(res_losses)\n",
    "    \n",
    "    min_val = np.min(loss_arr)\n",
    "    \n",
    "    min_flat_idx = np.argmin(loss_arr)\n",
    "    (min_train_setting_idx,min_train_num_idx) = divmod(min_flat_idx, loss_arr.shape[1])\n",
    "\n",
    "    best_params = res_params[min_train_setting_idx]\n",
    "    \n",
    "    best_params['best_sim_num'] = simulation_sequence_number[min_train_setting_idx][min_train_num_idx]\n",
    "    best_params['best_hum_num'] = human_sequence_number[min_train_setting_idx][min_train_num_idx]\n",
    "\n",
    "    best_params['min_loss'] = min_val\n",
    "    \n",
    "    return best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6b3ba7f-147d-4cee-93e0-4e22930c0e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "## These are the varieties of model types, training data types, and training input representation types that we want to find the best params for\n",
    "model_names = ['LSTM','GRU','Transformer']\n",
    "train_seq_parts = ['fix_only', 'fix_and_choice']\n",
    "fix_unit_types = ['ID', 'all']\n",
    "\n",
    "# These are the hyper-parameters that we want to vary / find the best of\n",
    "hidden_sizes = np.array([32, 64, 128, 256])\n",
    "sim_lrs = np.array([1e-4, 1e-3])\n",
    "human_lrs_train = np.array([1e-4, 1e-3])\n",
    "human_lrs_finetune = np.array([1e-5, 1e-4, 1e-3])\n",
    "\n",
    "# For the transformer only\n",
    "transformer_attention_heads = [4]\n",
    "transformer_layers = [2]\n",
    "\n",
    "n_runs = 2\n",
    "\n",
    "\n",
    "part_names = [\"Simulated_Only\", \"Human_Only\", \"Simulated_and_Human\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e295ab0-b3ae-4ea1-b377-46f90ebcad96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'simulation_loss_results': array([4.85965735, 4.83791438, 4.83987013, 4.8395671 , 4.83546928,\n",
       "         4.80007529, 4.72777243, 4.70001642, 4.61235152, 4.57637447,\n",
       "         4.65835556, 4.6756411 , 4.62488444, 4.57074903, 4.54084338,\n",
       "         4.57110047, 4.60072294, 4.56265923, 4.53306781, 4.56171766,\n",
       "         4.55471793, 4.52578311, 4.57631099, 4.53145914, 4.50021669,\n",
       "         4.51612635, 4.59120116, 4.5304355 , 4.51122344, 4.50346893,\n",
       "         4.49617356, 4.54740226, 4.48313949, 4.49795447, 4.49169941,\n",
       "         4.50472619, 4.61922103, 4.56994079, 4.49205108, 4.51842412,\n",
       "         4.52584043, 4.51278733, 4.54520623, 4.5544816 , 4.5031466 ,\n",
       "         4.51331602, 4.46537223, 4.54183449, 4.48968677, 4.47456452,\n",
       "         4.4782313 , 4.46404071, 4.47585484, 4.52545494, 4.49539779,\n",
       "         4.47536902, 4.4698094 , 4.48235159, 4.52275106, 4.47745813,\n",
       "         4.53714383, 4.50893974, 4.44910313, 4.43547544, 4.44947015,\n",
       "         4.45636815, 4.4297367 , 4.46171434, 4.4251087 , 4.60634892,\n",
       "         4.47933082, 4.54844891, 4.55403589, 4.43575613, 4.50626355,\n",
       "         4.47812435, 4.44995791, 4.43306166]),\n",
       "  'human_loss_results': array([5.70594752, 5.75336924, 5.75142258, 5.73364764, 5.71135017,\n",
       "         5.8240495 , 5.72431126, 5.66856602, 5.75295323, 5.81978446,\n",
       "         5.66546354, 5.65018055, 5.65279603, 5.7354883 , 5.83135736,\n",
       "         5.73699269, 5.6452142 , 5.73544595, 5.76903617, 5.68393484,\n",
       "         5.74203652, 5.79499859, 5.64313018, 5.72948942, 5.83146301,\n",
       "         5.85293573, 5.61540771, 5.71581322, 5.74923751, 5.86262169,\n",
       "         5.80610242, 5.66156885, 5.77765954, 5.71252173, 5.70052782,\n",
       "         5.68696424, 5.60383207, 5.63702139, 5.76282454, 5.67798567,\n",
       "         5.67074642, 5.68919724, 5.6667563 , 5.63145086, 5.76991788,\n",
       "         5.68652779, 5.88899377, 5.7084192 , 5.70332834, 5.7445296 ,\n",
       "         5.73085579, 5.75662744, 5.72737342, 5.66860622, 5.78332919,\n",
       "         5.74187839, 5.73598805, 5.70404318, 5.65898085, 5.71462187,\n",
       "         5.60885257, 5.65888843, 5.75091061, 5.80506119, 5.70613706,\n",
       "         5.70479405, 5.74756137, 5.70277646, 5.82933459, 5.56083566,\n",
       "         5.63230845, 5.57351878, 5.56049973, 5.6975579 , 5.60340038,\n",
       "         5.59237659, 5.74249864, 5.6668905 ]),\n",
       "  'train_sequence_number': array([  6432,  12832,  19232,  25632,  32032,  38432,  44832,  51232,\n",
       "          57632,  64032,  70432,  76832,  83232,  89632,  96032, 102432,\n",
       "         108832, 115232, 121632, 128032, 134432, 140832, 147232, 153632,\n",
       "         160032, 166432, 172832, 179232, 185632, 192032, 198432, 204832,\n",
       "         211232, 217632, 224032, 230432, 236832, 243232, 249632, 256032,\n",
       "         262432, 268832, 275232, 281632, 288032, 294432, 300832, 307232,\n",
       "         313632, 320032, 326432, 332832, 339232, 345632, 352032, 358432,\n",
       "         364832, 371232, 377632, 384032, 390432, 396832, 403232, 409632,\n",
       "         416032, 422432, 428832, 435232, 441632, 448032, 454432, 460832,\n",
       "         467232, 473632, 480032, 486432, 492832, 499232]),\n",
       "  'human_sequence_number': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'simulation_sequence_number': array([  6432,  12832,  19232,  25632,  32032,  38432,  44832,  51232,\n",
       "          57632,  64032,  70432,  76832,  83232,  89632,  96032, 102432,\n",
       "         108832, 115232, 121632, 128032, 134432, 140832, 147232, 153632,\n",
       "         160032, 166432, 172832, 179232, 185632, 192032, 198432, 204832,\n",
       "         211232, 217632, 224032, 230432, 236832, 243232, 249632, 256032,\n",
       "         262432, 268832, 275232, 281632, 288032, 294432, 300832, 307232,\n",
       "         313632, 320032, 326432, 332832, 339232, 345632, 352032, 358432,\n",
       "         364832, 371232, 377632, 384032, 390432, 396832, 403232, 409632,\n",
       "         416032, 422432, 428832, 435232, 441632, 448032, 454432, 460832,\n",
       "         467232, 473632, 480032, 486432, 492832, 499232]),\n",
       "  'r_sim_by_n_back': array([0.40276774, 0.37939535, 0.35446364, 0.31854415, 0.27675191,\n",
       "         0.26817398, 0.25237588, 0.23872888, 0.23887758, 0.23710699,\n",
       "         0.22643635, 0.22646632, 0.2299917 , 0.23058045, 0.2293933 ,\n",
       "         0.2384875 , 0.21991219, 0.20657613, 0.21760934]),\n",
       "  'pct_correct_max_sim_by_n_back': array([0.59375   , 0.546875  , 0.50984252, 0.458     , 0.38429752,\n",
       "         0.36123348, 0.3287037 , 0.3220339 , 0.34096692, 0.33513514,\n",
       "         0.34985423, 0.36170213, 0.40065147, 0.41868512, 0.43283582,\n",
       "         0.42857143, 0.3974359 , 0.37899543, 0.37948718]),\n",
       "  'pct_correct_min_sim_by_n_back': array([0.48828125, 0.48046875, 0.46653543, 0.472     , 0.4214876 ,\n",
       "         0.42731278, 0.42592593, 0.43341404, 0.4351145 , 0.45405405,\n",
       "         0.4548105 , 0.45592705, 0.45928339, 0.44636678, 0.43283582,\n",
       "         0.4484127 , 0.41025641, 0.42465753, 0.45641026]),\n",
       "  'pct_correct_order_sim_by_n_back': array([0.32617188, 0.30078125, 0.28149606, 0.266     , 0.21487603,\n",
       "         0.20044053, 0.18287037, 0.1937046 , 0.19847328, 0.21621622,\n",
       "         0.21282799, 0.23404255, 0.24429967, 0.23875433, 0.23507463,\n",
       "         0.23809524, 0.21794872, 0.20547945, 0.23076923]),\n",
       "  'r_human_by_n_back': array([0.21169076, 0.19443188, 0.18436547, 0.17456961, 0.15039209,\n",
       "         0.12841482, 0.1116935 , 0.08622932, 0.07307847, 0.06817443,\n",
       "         0.06683726, 0.0536791 , 0.0587146 , 0.08407002, 0.08321944,\n",
       "         0.07820319, 0.06815741, 0.05434712, 0.06587169]),\n",
       "  'pct_correct_max_human_by_n_back': array([0.54011742, 0.50687623, 0.5       , 0.5       , 0.47368421,\n",
       "         0.43006263, 0.41318681, 0.38479263, 0.33737864, 0.33066667,\n",
       "         0.3265896 , 0.30625   , 0.34722222, 0.38289963, 0.412     ,\n",
       "         0.37777778, 0.36231884, 0.36979167, 0.36312849]),\n",
       "  'pct_correct_min_human_by_n_back': array([0.44031311, 0.42043222, 0.40513834, 0.41269841, 0.40283401,\n",
       "         0.37995825, 0.38241758, 0.38479263, 0.37135922, 0.36266667,\n",
       "         0.37861272, 0.371875  , 0.375     , 0.39033457, 0.352     ,\n",
       "         0.36      , 0.352657  , 0.375     , 0.36312849]),\n",
       "  'pct_correct_order_human_by_n_back': array([0.28180039, 0.26522593, 0.26679842, 0.2718254 , 0.24291498,\n",
       "         0.21085595, 0.22417582, 0.2235023 , 0.17718447, 0.18133333,\n",
       "         0.18786127, 0.175     , 0.17708333, 0.19330855, 0.192     ,\n",
       "         0.19555556, 0.17391304, 0.1875    , 0.15642458])},\n",
       " {'part_name': 'Simulated_Only',\n",
       "  'train_seq_part': 'fix_only',\n",
       "  'fu': 'ID',\n",
       "  'model_name': 'LSTM',\n",
       "  'd_model': 32,\n",
       "  'sim_lr': 0.001,\n",
       "  'human_lr': 0,\n",
       "  'n_head': 0,\n",
       "  'n_layers': 0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_results(0, part_names[0],train_seq_parts[0], fix_unit_types[0], model_names[0], hidden_sizes[0], sim_lrs[1], 0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "517e4d97-aaa8-4aab-913c-71a016aa1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get loss curve for each model / train_seq_part / model_name / part_name\n",
    "\n",
    "res_params = {}\n",
    "res_losses = {}\n",
    "res_human_seq_nums = {}\n",
    "res_sim_seq_nums = {}\n",
    "\n",
    "# store all parameters and loss curves... \n",
    "for fu in fix_unit_types:\n",
    "    for tsp in train_seq_parts:\n",
    "        for part_name in part_names:\n",
    "            for model_name in model_names:\n",
    "                full_name = \"{}_{}_{}_{}\".format(tsp, part_name, model_name, fu)\n",
    "                res_params[full_name] = []\n",
    "                res_losses[full_name] = []\n",
    "                res_human_seq_nums[full_name] = []\n",
    "                res_sim_seq_nums[full_name] = []\n",
    "\n",
    "                if part_name == 'Simulated_Only':\n",
    "                    full_name = \"{}_{}_{}_{}\".format(tsp, 'Simulated_Only_Pred_Human', model_name, fu)\n",
    "                    res_params[full_name] = []\n",
    "                    res_losses[full_name] = []\n",
    "                    res_human_seq_nums[full_name] = []\n",
    "                    res_sim_seq_nums[full_name] = []\n",
    "            \n",
    "# for simulated only -- find both the params that maximize simulated data performance \n",
    "# and also the params that maximize human performance\n",
    "for fu in fix_unit_types:\n",
    "    for model_name in model_names:\n",
    "        for tsp in train_seq_parts:\n",
    "            for part_name in part_names:\n",
    "                \n",
    "                # loop through params that we we want to max over...\n",
    "                for d_model in hidden_sizes:\n",
    "\n",
    "                    if part_name == 'Simulated_Only':\n",
    "                        for sim_lr in sim_lrs:\n",
    "                            \n",
    "                            human_lr = 0\n",
    "                            \n",
    "                            if model_name == 'Transformer':\n",
    "                                for n_layers in transformer_layers:\n",
    "                                    for n_head in transformer_attention_heads:\n",
    "                                        \n",
    "                                        # get the simulation loss...\n",
    "                                        mean_loss, sem_loss, train_sequence_number,simulation_sequence_number, human_sequence_number, this_params  = get_learning_curve(part_name,tsp, fu, model_name, d_model, sim_lr, human_lr, n_head, n_layers, n_runs = 2, which_loss = 'simulation_loss_results')\n",
    "                                        full_name = \"{}_{}_{}_{}\".format(tsp, part_name, model_name, fu)\n",
    "\n",
    "                                        res_losses[full_name].append(mean_loss)\n",
    "                                        res_params[full_name].append(this_params)\n",
    "                                        \n",
    "                                        res_human_seq_nums[full_name].append(human_sequence_number)\n",
    "                                        res_sim_seq_nums[full_name].append(simulation_sequence_number)\n",
    "\n",
    "                                        \n",
    "                                        # get the human loss...\n",
    "                                        mean_loss, sem_loss, train_sequence_number,simulation_sequence_number, human_sequence_number, this_params  = get_learning_curve(part_name,tsp, fu, model_name, d_model, sim_lr, human_lr, n_head, n_layers, n_runs = 2, which_loss = 'human_loss_results')\n",
    "                                        full_name = \"{}_{}_{}_{}\".format(tsp, 'Simulated_Only_Pred_Human', model_name, fu)\n",
    "                                        res_losses[full_name].append(mean_loss)\n",
    "                                        res_params[full_name].append(this_params)\n",
    "                                        \n",
    "                                        res_human_seq_nums[full_name].append(human_sequence_number)\n",
    "                                        res_sim_seq_nums[full_name].append(simulation_sequence_number)\n",
    "\n",
    "        \n",
    "                            else: # not transformer\n",
    "                                n_head = 0\n",
    "                                n_layers = 0   \n",
    "                        \n",
    "                                # get the simulation loss...\n",
    "                                mean_loss, sem_loss, train_sequence_number,simulation_sequence_number, human_sequence_number, this_params  = get_learning_curve(part_name,tsp, fu, model_name, d_model, sim_lr, human_lr, n_head, n_layers, n_runs = 2, which_loss = 'simulation_loss_results')\n",
    "                                full_name = \"{}_{}_{}_{}\".format(tsp, part_name, model_name, fu)\n",
    "\n",
    "                                res_losses[full_name].append(mean_loss)\n",
    "                                res_params[full_name].append(this_params)\n",
    "                                \n",
    "                                res_human_seq_nums[full_name].append(human_sequence_number)\n",
    "                                res_sim_seq_nums[full_name].append(simulation_sequence_number)\n",
    "\n",
    "                                # get the human loss...\n",
    "                                mean_loss, sem_loss, train_sequence_number,simulation_sequence_number, human_sequence_number, this_params  = get_learning_curve(part_name,tsp, fu, model_name, d_model, sim_lr, human_lr, n_head, n_layers, n_runs = 2, which_loss = 'human_loss_results')\n",
    "                                full_name = \"{}_{}_{}_{}\".format(tsp, 'Simulated_Only_Pred_Human', model_name, fu)\n",
    "                                res_losses[full_name].append(mean_loss)\n",
    "                                res_params[full_name].append(this_params)\n",
    "                                \n",
    "                                res_human_seq_nums[full_name].append(human_sequence_number)\n",
    "                                res_sim_seq_nums[full_name].append(simulation_sequence_number)\n",
    "                                \n",
    "                                \n",
    "                    else: # not Sim Only\n",
    "                        \n",
    "                        if part_name == 'Human_Only':\n",
    "                            sim_lr = 0\n",
    "                            these_human_lrs = human_lrs_train\n",
    "                            \n",
    "                        else:\n",
    "                            sim_lr = .001\n",
    "                            these_human_lrs = human_lrs_finetune\n",
    "                            \n",
    "                        \n",
    "                        for human_lr in these_human_lrs:\n",
    "\n",
    "                            \n",
    "                            \n",
    "                            if model_name == 'Transformer':\n",
    "\n",
    "                                for n_layers in transformer_layers:\n",
    "                                    for n_head in transformer_attention_heads:\n",
    "\n",
    "                                        # get the human loss...\n",
    "                                        mean_loss, sem_loss, train_sequence_number,simulation_sequence_number, human_sequence_number, this_params  = get_learning_curve(part_name,tsp, fu, model_name, d_model, sim_lr, human_lr, n_head, n_layers, n_runs = 2, which_loss = 'human_loss_results')\n",
    "                                        full_name = \"{}_{}_{}_{}\".format(tsp, part_name, model_name, fu)\n",
    "\n",
    "                                        res_losses[full_name].append(mean_loss)\n",
    "                                        res_params[full_name].append(this_params)\n",
    "                                        res_human_seq_nums[full_name].append(human_sequence_number)\n",
    "                                        res_sim_seq_nums[full_name].append(simulation_sequence_number)\n",
    "\n",
    "                            else: # not a transformer\n",
    "\n",
    "                                n_head = 0\n",
    "                                n_layers = 0   \n",
    "\n",
    "                                # get the simulation loss...\n",
    "                                mean_loss, sem_loss, train_sequence_number,simulation_sequence_number, human_sequence_number, this_params  = get_learning_curve(part_name,tsp, fu, model_name, d_model, sim_lr, human_lr, n_head, n_layers, n_runs = 2, which_loss = 'human_loss_results')\n",
    "                                full_name = \"{}_{}_{}_{}\".format(tsp, part_name, model_name, fu)\n",
    "\n",
    "                                res_losses[full_name].append(mean_loss)\n",
    "                                res_params[full_name].append(this_params)\n",
    "                                res_human_seq_nums[full_name].append(human_sequence_number)\n",
    "                                res_sim_seq_nums[full_name].append(simulation_sequence_number)\n",
    "                            \n",
    "                            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17c98923-ead0-46ad-95f5-785fbbfdd59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_seq = {}\n",
    "\n",
    "for fu in fix_unit_types:\n",
    "    for tsp in train_seq_parts:\n",
    "        for part_name in part_names:\n",
    "            for model_name in model_names:\n",
    "                \n",
    "                full_name = \"{}_{}_{}_{}\".format(tsp, part_name, model_name, fu)\n",
    "                \n",
    "                these_best_params = get_best_params(res_losses[full_name],res_params[full_name], res_sim_seq_nums[full_name], res_human_seq_nums[full_name])\n",
    "                best_params_seq[full_name] = these_best_params\n",
    "\n",
    "                if part_name == 'Simulated_Only':\n",
    "                    full_name = \"{}_{}_{}_{}\".format(tsp, 'Simulated_Only_Pred_Human', model_name, fu)\n",
    "                    these_best_params = get_best_params(res_losses[full_name],res_params[full_name], res_sim_seq_nums[full_name], res_human_seq_nums[full_name])\n",
    "                    best_params_seq[full_name] = these_best_params\n",
    "                    \n",
    "# save\n",
    "f = open(os.path.join(best_param_folder, \"best_hyper_params.pkl\"),\"wb\")\n",
    "\n",
    "# write the python object (dict) to pickle file\n",
    "pickle.dump(best_param_dict,f)\n",
    "\n",
    "# close file\n",
    "f.close()                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
