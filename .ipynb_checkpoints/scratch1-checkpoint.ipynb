{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13cb7eab-0e46-425c-92ac-d3db8ea18be7",
   "metadata": {},
   "source": [
    "## 1. Structure Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a02a033d-2d6d-4736-a44f-f60b900d05c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# funcs stolen from nyu deep learning course\n",
    "from res.sequential_tasks import pad_sequences, to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5a3625e9-6450-4d54-b731-446fc6306300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train and test datasets...\n",
    "\n",
    "train_data_file_path = 'simulated_data/train_data_v1.0.json'\n",
    "train_data_file = open(train_data_file_path)\n",
    "train_sim_data = json.load(train_data_file)\n",
    "\n",
    "test_data_file_path = 'simulated_data/test_data_v1.0.json'\n",
    "test_data_file = open(test_data_file_path)\n",
    "test_sim_data = json.load(test_data_file)\n",
    "\n",
    "# see this for loading just part of a json if we run into memory issues\n",
    "# https://stackoverflow.com/questions/32661646/load-part-of-a-json-in-python\n",
    "\n",
    "# sim_data is a list of dicts\n",
    "# each dict has 'values' (3x1), fixations (many lengths), and choice (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4fd424f1-e89f-4bb4-be56-a1ffe5018b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch_data(batch_size, batch_idx, sim_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create sequence and target data for a batch\n",
    "    \n",
    "    Input: \n",
    "        batch_size: number of trials to include in batch\n",
    "        batch_idx: index of data\n",
    "        sim_data: list of dicts, where each dict has 'values', 'fixations', and 'choice'\n",
    "        \n",
    "    Returns:\n",
    "        a tuple, (batch_data, batch_targets)\n",
    "        batch_data is 3d array: batch_size x sequence_size x one-hot categorical encoding (3 here)\n",
    "        batch_targets is 2d array: \n",
    "    \"\"\"\n",
    "\n",
    "    # filter list of trials that are in this batch\n",
    "    batch_sim_data = sim_data[batch_idx*batch_size:((batch_idx+1)*(batch_size))]\n",
    "    \n",
    "    ## generate sequences of fixations + choice\n",
    "\n",
    "    # all sequences in the batch, attended item is coded as idx (as 0, 1, 2)\n",
    "    batch_sequences_idx = [trial_data['fixations'] + [trial_data['choice']] for trial_data in batch_sim_data]\n",
    "\n",
    "    # all sequences in the batch, attended item coded as one-hot categorical: e.g. 0: [1,0,0] 1: [0,1,0], [0,0,1]\n",
    "    batch_sequences_cat = [[to_categorical(x, num_classes = 3) for x in this_sequence] for this_sequence in batch_sequences_idx]\n",
    "\n",
    "    # pad front of each sequence with n x [0,0,0] so that all seqeunces are same length\n",
    "    batch_data = pad_sequences(batch_sequences_cat)\n",
    "\n",
    "    ## generate sequences of targets\n",
    "    batch_targets = np.array([trial_data['values'] for trial_data in batch_sim_data], dtype = 'float32')\n",
    "    \n",
    "    return (batch_data, batch_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f3a2cc6b-15a6-4ca2-ba52-0d90f703cad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The return type is a <class 'tuple'> with length 2.\n",
      "The first item in the tuple is the batch of sequences with shape (32, 23, 3).\n",
      "The first element in the batch of sequences is:\n",
      " [[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]]\n",
      "The second item in the tuple is the corresponding batch of class labels with shape (32, 3).\n",
      "The first element in the batch of targets is:\n",
      " [5. 6. 3.]\n"
     ]
    }
   ],
   "source": [
    "example_batch = gen_batch_data(32, 0, train_sim_data) # batch size = 32, idx = 0\n",
    "print(f'The return type is a {type(example_batch)} with length {len(example_batch)}.')\n",
    "print(f'The first item in the tuple is the batch of sequences with shape {example_batch[0].shape}.')\n",
    "print(f'The first element in the batch of sequences is:\\n {example_batch[0][0, :, :]}')\n",
    "print(f'The second item in the tuple is the corresponding batch of class labels with shape {example_batch[1].shape}.')\n",
    "print(f'The first element in the batch of targets is:\\n {example_batch[1][0, :]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616a57c2-50ba-4d0d-ba0f-d3b9dd436111",
   "metadata": {},
   "source": [
    "## 2. Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f86e602c-b396-4abe-aab2-0ecf1f4ea6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set the random seed for reproducible results\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # This just calls the base class constructor\n",
    "        super().__init__()\n",
    "        # Neural network layers assigned as attributes of a Module subclass\n",
    "        # have their parameters registered for training automatically.\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, nonlinearity='relu', batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The RNN also returns its hidden state but we don't use it.\n",
    "        # While the RNN can also take a hidden state as input, the RNN\n",
    "        # gets passed a hidden state initialized with zeros by default.\n",
    "        h = self.rnn(x)[0]\n",
    "        x = self.linear(h)\n",
    "        return x\n",
    "    \n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.lstm(x)[0]\n",
    "        x = self.linear(h)\n",
    "        return x\n",
    "    \n",
    "    def get_states_across_time(self, x):\n",
    "        h_c = None\n",
    "        h_list, c_list = list(), list()\n",
    "        with torch.no_grad():\n",
    "            for t in range(x.size(1)):\n",
    "                h_c = self.lstm(x[:, [t], :], h_c)[1]\n",
    "                h_list.append(h_c[0])\n",
    "                c_list.append(h_c[1])\n",
    "            h = torch.cat(h_list)\n",
    "            c = torch.cat(c_list)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0d69b7-62b6-4660-a1f0-f4583ce49644",
   "metadata": {},
   "source": [
    "## 3. Defining the Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "716c3703-b5cc-4758-9a82-de3ca991514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data_gen, criterion, optimizer, device):\n",
    "    # Set the model to training mode. This will turn on layers that would\n",
    "    # otherwise behave differently during evaluation, such as dropout.\n",
    "    model.train()\n",
    "\n",
    "    # Store the number of sequences that were classified correctly\n",
    "    num_correct = 0\n",
    "\n",
    "    # Iterate over every batch of sequences. Note that the length of a data generator\n",
    "    # is defined as the number of batches required to produce a total of roughly 1000\n",
    "    # sequences given a batch size.\n",
    "    for batch_idx in range(len(train_data_gen)):\n",
    "\n",
    "        # Request a batch of sequences and class labels, convert them into tensors\n",
    "        # of the correct type, and then send them to the appropriate device.\n",
    "        data, target = train_data_gen[batch_idx]\n",
    "        data, target = torch.from_numpy(data).float().to(device), torch.from_numpy(target).long().to(device)\n",
    "\n",
    "        # Perform the forward pass of the model\n",
    "        output = model(data)  # Step ①\n",
    "\n",
    "        # Pick only the output corresponding to last sequence element (input is pre padded)\n",
    "        output = output[:, -1, :]\n",
    "\n",
    "        # Compute the value of the loss for this batch. For loss functions like CrossEntropyLoss,\n",
    "        # the second argument is actually expected to be a tensor of class indices rather than\n",
    "        # one-hot encoded class labels. One approach is to take advantage of the one-hot encoding\n",
    "        # of the target and call argmax along its second dimension to create a tensor of shape\n",
    "        # (batch_size) containing the index of the class label that was hot for each sequence.\n",
    "        target = target.argmax(dim=1)\n",
    "\n",
    "        loss = criterion(output, target)  # Step ②\n",
    "\n",
    "        # Clear the gradient buffers of the optimized parameters.\n",
    "        # Otherwise, gradients from the previous batch would be accumulated.\n",
    "        optimizer.zero_grad()  # Step ③\n",
    "\n",
    "        loss.backward()  # Step ④\n",
    "\n",
    "        optimizer.step()  # Step ⑤\n",
    "        \n",
    "        y_pred = output.argmax(dim=1)\n",
    "        \n",
    "        # this is wrong since we're doing regression...\n",
    "        num_correct += (y_pred == target).sum().item()\n",
    "\n",
    "    return num_correct, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf98346-9e9c-4e3b-b4f8-8429df8df956",
   "metadata": {},
   "source": [
    "## 4. Defining the Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6b6f396f-0d7f-48a8-8ee7-ed0aa1e2ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_data_gen, criterion, device):\n",
    "    # Set the model to evaluation mode. This will turn off layers that would\n",
    "    # otherwise behave differently during training, such as dropout.\n",
    "    model.eval()\n",
    "\n",
    "    # Store the number of sequences that were classified correctly\n",
    "    num_correct = 0\n",
    "\n",
    "    # A context manager is used to disable gradient calculations during inference\n",
    "    # to reduce memory usage, as we typically don't need the gradients at this point.\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(len(test_data_gen)):\n",
    "            data, target = test_data_gen[batch_idx]\n",
    "            data, target = torch.from_numpy(data).float().to(device), torch.from_numpy(target).long().to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            # Pick only the output corresponding to last sequence element (input is pre padded)\n",
    "            output = output[:, -1, :]\n",
    "\n",
    "            #target = target.argmax(dim=1)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            y_pred = output.argmax(dim=1)\n",
    "            # num_correct += (y_pred == target).sum().item()\n",
    "\n",
    "    #return num_correct, loss.item()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca2fde-1b09-4b97-8715-b07e74aa6407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
